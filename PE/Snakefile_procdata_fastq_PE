import os
import glob
import pandas as pd
import itertools

#from ordered_set import OrderedSet


## version 2.0
## 30 Jan 2024 - 1 Feb 2024


# Config file
configfile: "config.yml"

# Paths
datadir = config["datadir"]

resdir= config["resdir"]

fastqcdir1 = resdir + "fastqc_raw/"
fastqcdir2 = resdir + "fastqc_proc/"
multiqcdir = resdir + "multiqc/"
picarddir= resdir + "picard/"
picarddir_uniq= picarddir + "unique_bestmap/"
picarddir_multi= picarddir + "multi_randompos/"

trimmeddir= resdir + "cutadapt/"
bwt_idx_dir = resdir + "genome_idx_bowtie/"
mappeddir= resdir + "mapped_bowtie/"
bwt_multi= mappeddir + "multi_randompos/"
bwt_uniq= mappeddir + "unique_bestmap/"

deeptoolsdir= resdir + "tracks_deepTools/"
bw_uniq=deeptoolsdir + "bw_cov1x_unique_bestmap/"
bw_multi=deeptoolsdir + "bw_cov1x_multi_randompos/"

logdir = config["logdir"]


# Get samples names
# Get samples & file bases in the fastq data directory
# Get samples & file bases in the fastq data directory
fastq_base = glob.glob(datadir + "*_R1.fastq.gz")                  
fastq_base = [s.replace('_R1.fastq.gz', '') for s in fastq_base]
fastq_pths = fastq_base #list of input fastq files with Paths       
fastq_base = [s.replace(datadir, '') for s in fastq_base]

samples_base = list(set(fastq_base))



#####################
## TMPDIR

# TMPDIR needs to be exported before running the pipeline or otherwise properly configured

#####################


rule all:
    input:
        resdir + "rulegraph.png",
        resdir + "dag.png",
        expand(fastqcdir1+ "{id}_R1_fastqc.html",id=samples_base),
        expand(fastqcdir1+ "{id}_R2_fastqc.html",id=samples_base),
        expand(fastqcdir2+ "{id}.trimmed_R1_fastqc.html",id=samples_base),
        expand(fastqcdir2+ "{id}.trimmed_R2_fastqc.html",id=samples_base),
        expand(bw_uniq + "{id}.bwt.unique_bestmap.cov1x.bw",id=samples_base),
        expand(bw_multi + "{id}.bwt.multi_randompos.cov1x.bw",id=samples_base),
        expand(picarddir_multi + "{id}.bwt.multi_randompos.insert_size_histogram.pdf",id=samples_base),        
        expand(picarddir_uniq + "{id}.bwt.unique_bestmap.insert_size_histogram.pdf",id=samples_base),        
        multiqcdir + "multiqc_report.html"


rule bowtie_idx:
    input:
        config["GENOME_FA"]
    output:
        idx= bwt_idx_dir + "Sce_s288c.1.ebwt",
        idxrev= bwt_idx_dir + "Sce_s288c.rev.1.ebwt"
    log:
        logdir + "bwt_idx.log"
    shell:
        """
        pth="{output.idx}"
        #bowtie-build --threads 2 -f {input} ${{pth%.1.ebwt}} &> {log}
        
        #bowtie 1.2.0
        bowtie-build -f {input} ${{pth%.1.ebwt}} &> {log}

        """



rule fastqc_raw:
    input:
        t1=datadir + "{id}_R1.fastq.gz",
        t2=datadir + "{id}_R2.fastq.gz"
    output:
        html1= fastqcdir1 + "{id}_R1_fastqc.html",
        ziparch1= fastqcdir1 + "{id}_R1_fastqc.zip",
        html2= fastqcdir1 + "{id}_R2_fastqc.html",
        ziparch2= fastqcdir1 + "{id}_R2_fastqc.zip"
    shell:
        """
        # Run FastQC and save the output to the current directory
        fastqc {input.t1} -q -o $(dirname {output.html1})
        fastqc {input.t2} -q -o $(dirname {output.html2})
        """


rule cutadapt:
    input:
        r1=datadir + "{id}_R1.fastq.gz",
        r2=datadir + "{id}_R2.fastq.gz"
    output:
        fastq1=trimmeddir + "{id}.trimmed_R1.fastq.gz",
        log= logdir + "{id}.cutadapt.log",
        fastq2=trimmeddir + "{id}.trimmed_R2.fastq.gz",
    shell:
        """
        cutadapt -a {config[adapterFWD]} -A {config[adapterREV]} -l 140 -m 30 -j 0 -q 20 --trim-n --nextseq-trim=20 -o {output.fastq1} -p {output.fastq2} {input.r1} {input.r2} &> {output.log}
        """

rule fastqc_proc:
    input:
        t1=trimmeddir + "{id}.trimmed_R1.fastq.gz",
        t2=trimmeddir + "{id}.trimmed_R2.fastq.gz"
    output:
        html1= fastqcdir2 + "{id}.trimmed_R1_fastqc.html",
        ziparch1= fastqcdir2 + "{id}.trimmed_R1_fastqc.zip",
        html2= fastqcdir2 + "{id}.trimmed_R2_fastqc.html",
        ziparch2= fastqcdir2 + "{id}.trimmed_R2_fastqc.zip"
    shell:
        """
        # Run FastQC and save the output to the current directory
        fastqc {input.t1} -q -o $(dirname {output.html1})
        fastqc {input.t2} -q -o $(dirname {output.html2})
        """




rule map_bowtie_unique_bestmap:
    input:
        fq1=trimmeddir + "{id}.trimmed_R1.fastq.gz",
        fq2=trimmeddir + "{id}.trimmed_R2.fastq.gz",
        idx= bwt_idx_dir + "Sce_s288c.1.ebwt",
        idxrev= bwt_idx_dir + "Sce_s288c.rev.1.ebwt"
    output:
        log= logdir + "{id}.bowtie.unique_bestmap.log",
        bam= bwt_uniq + "{id}.bwt.unique_bestmap.sorted.bam"
    shell:
        """
        pth="{input.idx}"
        foo="{input.idxrev}"

        bowtie -n 3 -m 1 --best --strata -a -q -S -p 16 -y --fr -t -X {config[insertSize]} --chunkmbs 512 -x ${{pth%.1.ebwt}} -1 {input.fq1} -2 {input.fq2} $TMPDIR/{wildcards.id}.ub.bam 2> {output.log} 

        samtools sort -T $TMPDIR/{wildcards.id}.ub.tmp -o {output.bam} $TMPDIR/{wildcards.id}.ub.bam
        
        samtools index {output.bam}
        """


rule map_bowtie_multi_randompos:
    input:
        fq1=trimmeddir + "{id}.trimmed_R1.fastq.gz",
        fq2=trimmeddir + "{id}.trimmed_R2.fastq.gz",
        idx= bwt_idx_dir + "Sce_s288c.1.ebwt",
        idxrev= bwt_idx_dir + "Sce_s288c.rev.1.ebwt"
    output:
        log= logdir + "{id}.bowtie.multi_randompos.log",
        bam= bwt_multi + "{id}.bwt.multi_randompos.sorted.bam"
    shell:
        """
        pth="{input.idx}"
        foo="{input.idxrev}"

        bowtie -n 3 -M 1 --best --strata -a -q -S -p 16 -y --fr -t -X {config[insertSize]} --chunkmbs 512 -x ${{pth%.1.ebwt}} -1 {input.fq1} -2 {input.fq2} $TMPDIR/{wildcards.id}.mr.bam  2> {output.log}

        samtools sort -T $TMPDIR/{wildcards.id}.mr.tmp -o {output.bam} $TMPDIR/{wildcards.id}.mr.bam
        
        samtools index {output.bam}
        """


rule picard_multi:
    input:
        bam= bwt_multi + "{id}.bwt.multi_randompos.sorted.bam"
    output:
        metrics= picarddir_multi + "{id}.bwt.multi_randompos.insert_size_metrics.txt",
        hist= picarddir_multi + "{id}.bwt.multi_randompos.insert_size_histogram.pdf"
    shell:
        """
        java -jar picard.jar CollectInsertSizeMetrics -I {input.bam} -O {output.metrics} -H {output.hist} -M 0.5
        """


rule picard_uniq:
    input:
        bam= bwt_uniq + "{id}.bwt.unique_bestmap.sorted.bam"
    output:
        metrics= picarddir_uniq + "{id}.bwt.unique_bestmap.insert_size_metrics.txt",
        hist= picarddir_uniq + "{id}.bwt.unique_bestmap.insert_size_histogram.pdf"
    shell:
        """
        java -jar picard.jar CollectInsertSizeMetrics -I {input.bam} -O {output.metrics} -H {output.hist} -M 0.5
        """



rule deeptools_bw_multi:
    input:
        bam= bwt_multi + "{id}.bwt.multi_randompos.sorted.bam"
    output:
        bw= bw_multi + "{id}.bwt.multi_randompos.cov1x.bw"
    shell:
        """
        bamCoverage -p 1 --bam {input.bam} --outFileName {output.bw} --normalizeUsing RPGC --exactScaling --effectiveGenomeSize {config[efGenomeSize]} --extendReads --binSize 1 --outFileFormat bigwig --ignoreForNormalization {config[normIgnore]} &>/dev/null
        """


rule deeptools_bw_unique:
    input:
        bam= bwt_uniq + "{id}.bwt.unique_bestmap.sorted.bam"
    output:
        bw= bw_uniq + "{id}.bwt.unique_bestmap.cov1x.bw"
    shell:
        """
        bamCoverage -p 1 --bam {input.bam} --outFileName {output.bw} --normalizeUsing RPGC --exactScaling --effectiveGenomeSize {config[efGenomeSize]} --extendReads --binSize 1 --outFileFormat bigwig --ignoreForNormalization {config[normIgnore]} &>/dev/null
        """


################################
## final rules
################################

rule generate_rulegraph:
    """
    Generate a rulegraph for the workflow.
    """
    output:
        resdir + "rulegraph.png",
        resdir + "dag.png"
    shell:
        """
        snakemake --snakefile {config[snkpth]} --config max_reads=0 --rulegraph | dot -Tpng >{output[0]}
        snakemake --snakefile {config[snkpth]} --config max_reads=0 --dag | dot -Tpng >{output[1]}
        """

rule MultiQC:
    input:
        expand(fastqcdir1+ "{id}_R1_fastqc.html",id=samples_base),
        expand(fastqcdir1+ "{id}_R2_fastqc.html",id=samples_base),
        expand(fastqcdir2+ "{id}.trimmed_R1_fastqc.html",id=samples_base),
        expand(fastqcdir2+ "{id}.trimmed_R2_fastqc.html",id=samples_base),
        expand(logdir + "{id}.bowtie.unique_bestmap.log", id=samples_base),
        expand(logdir + "{id}.bowtie.multi_randompos.log", id=samples_base)
    output:
        multiqcdir + "multiqc_report.html"
    shell:
        """
        multiqc -f --outdir $(dirname {output}) {resdir} {logdir}        
        """

